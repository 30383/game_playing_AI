# game_playing_AI
Contains codes for training and usage of AI for playing games using evolutionary and reinforcement algorithms.

The project is primarily based on the NeuroEvolution of Augmented Topologies (NEAT) algorithm and its application and the proposed improvement, all for the purpose of game playing AI agent. The NEAT algorithm is gaining popularity due to the recent advancements in processing powers, which enables the algorithm to combine the evolutionary algorithms with the popular deep neural networks. Genetic algorithms is the most popular type of evolutionary algorithm and the approach used in NEAT due to the principles of natural selection it is based on.
Genetic algorithms work by the principle of “Survival of the fittest”. A population of1 candidate solutions is maintained, genetic operations such as mutations, cross-over are applied on them and through the defined fitness function, the fitness of each genome (candidate solution, neural network in our case) are evaluated. The best performing candidates is chosen as the base neural network for the next generation and the same genetic operations are applied. NeuroEvolution uses genetic algorithms on neural networks, to optimize the structure and other parameters such as weights and biases of the ANN. It is a hybrid between neural network and genetic algorithm. If we choose to evolve both the structure of the networks and their weights, the sub class is known as TWEANN (Topology and Weight Evolving ANN). NEAT is an example of TWEANN. Mutations with respect to NEAT correspond to either adding a connection between two unconnected nodes or adding a node on an existing connection (A–>B : A–>C–>B). Genomes whose structures are closely related, as in if the neural network structures resemble each other to a high extent, are grouped together to form a single species. This is decided by the genomic distance, a hyperparameter in NEAT.
	An extension to the NEAT algorithm came up, known as HyperNEAT (Hypercube-based NEAT), to tackle the issue of high dimensional input. 
	The proposed improvement or recommendation of algorithm over NEAT algorithm to tackle high number of inputs and more complex games such as Mario is the Double Deep Q Network algorithm. The DDQN is a reinforcement learning algorithm which combines Q-learning, which is an algorithm for learning the optimal actions in an environment with deep neural networks. Since it involves using two separate Q-value estimators to tackle the issue of overestimating, each of which is used to update the other (Double Q-learning), the algorithm is called Double Deep Q-Network. One set of Q-values (Q1) is used to determine the best action and the other set of Q-values (Q2) is used to evaluate that action’s quality. This reduces the overestimation problem in Q-learning.

 We have implemented the paper [[NEAT algorithm for simple games]((https://ieeexplore.ieee.org/document/10193858))], which served as the starting point for our project. 
